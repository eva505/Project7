{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a97bf5",
   "metadata": {},
   "source": [
    " # Project 7 - Implémentation d'un Modèle de Scoring\n",
    " \n",
    " ## Part 1 of XX - Prétraitment y inclu Aggrégation et Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a692d",
   "metadata": {},
   "source": [
    "# 1. Kaggle Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e933ad",
   "metadata": {},
   "source": [
    "**Data:**\n",
    "* https://www.kaggle.com/c/home-credit-default-risk/data\n",
    "\n",
    "**Series of Notebooks detailing the Problem:**\n",
    "* https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n",
    "* https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering\n",
    "* https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2\n",
    "* ...\n",
    "\n",
    "**EDA:**\n",
    "* https://www.kaggle.com/gpreda/home-credit-default-risk-extensive-eda\n",
    "\n",
    "**Feature Engineering:**\n",
    "* https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features\n",
    "\n",
    "**Models/Scripts - Light_GBM**\n",
    "* https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "* https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm\n",
    "\n",
    "**DNN:**\n",
    "https://www.kaggle.com/shep312/deep-learning-in-tf-with-upsampling-lb-758\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b897b8f",
   "metadata": {},
   "source": [
    "# 2. Imports Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0857a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3631eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b54d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import win32com.client "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400c7a0",
   "metadata": {},
   "source": [
    "# 3. Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403947a",
   "metadata": {},
   "source": [
    "## 3.1 Traitement des Features Catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a8132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    orig_columns = df.columns\n",
    "    cat_columns = df.select_dtypes(include = 'object').columns\n",
    "    df = pd.get_dummies(df, columns=cat_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in orig_columns]\n",
    "    #Apply re.sub and replace method to remove punctuation symbols and transform spaces into underscores.\n",
    "    df.rename(columns={col : re.sub(r'[^\\w\\s]', '', col).replace(' ','_') for col in new_columns}, inplace=True)\n",
    "    new_columns = [c for c in df.columns if c not in orig_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884005c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encoder(series, cat_type=None, impute=True, impute_value=None,):\n",
    "    \n",
    "    if cat_type == 'education':\n",
    "        repl_dict = {'lower secondary' : 0,\n",
    "                     'secondary / secondary special' : 1, \n",
    "                     'incomplete higher' : 2, \n",
    "                     'higher education' : 3,\n",
    "                     'academic degree' : 4}\n",
    "    elif cat_type == 'weekday':\n",
    "        repl_dict = {'monday' : 0,\n",
    "                     'tuesday' : 1, \n",
    "                     'wednesday' : 2, \n",
    "                     'thursday' : 3,\n",
    "                     'friday' : 4,\n",
    "                     'saturday' : 5,\n",
    "                     'sunday' : 6}\n",
    "    elif cat_type == 'yes_no' :\n",
    "        list_one  = ['y', 'yes', 'true', 't']\n",
    "        list_zero = ['n', 'no', 'false', 'f']\n",
    "        repl_dict = {i:1 for i in list_one} | {i:0 for i in list_zero}\n",
    "    elif cat_type == 'gender' :\n",
    "        list_one  = ['f', 'fem', 'female', 'w', 'woman']\n",
    "        list_zero = ['m', 'man', 'male']\n",
    "        repl_dict = {i:1 for i in list_one} | {i:0 for i in list_zero}\n",
    "    elif cat_type == 'loan' :\n",
    "        repl_dict = {'cash loans':1,\n",
    "                     'revolving loans':0,}\n",
    "    elif cat_type == 'currency':\n",
    "        list_one  = ['currency 1']\n",
    "        list_zero = ['currency 2', 'currency 3', 'currency 4']\n",
    "        repl_dict = {i:1 for i in list_one} | {i:0 for i in list_zero}\n",
    "    elif cat_type == 'yield_group' :\n",
    "        repl_dict = {'low_action' : 1,\n",
    "                     'low_normal' : 2,\n",
    "                     'middle' : 3,\n",
    "                     'high' : 4}\n",
    "    else:\n",
    "        le = LabelEncoder()\n",
    "        s = le.fit_transform(series)\n",
    "        s = pd.Series(s, name=series.name)\n",
    "        repl_dict = {le.inverse_transform([v])[0]:v for v in s.unique()}\n",
    "        return s\n",
    "    \n",
    "    s = series.str.lower().replace(repl_dict)\n",
    "    if impute and (len(s.unique()) > len(set(repl_dict.values()))) :\n",
    "        if impute_value == None:\n",
    "            impute_value = np.nan\n",
    "        s = pd.Series([impute_value if value not in repl_dict.values() else value for value in s], name=series.name)\n",
    "    return s, repl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be72026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_cat_reduction(series, cat_type=None):\n",
    "    if cat_type == 'bureau_credit_type':\n",
    "        categories =['Consumer credit', 'Credit card', 'Mortgage', 'Car loan',\n",
    "                     'Microloan', 'Loan for business development', 'Unknown type of loan',\n",
    "                     'Another type of loan']\n",
    "        other_value = 'Another type of loan'\n",
    "        s = series.fillna('Unknown type of loan')\n",
    "        s = pd.Series([v if v in categories else other_value for v in s], name=series.name)\n",
    "        return s\n",
    "    elif cat_type == 'bureau_balance_status':\n",
    "        cat_dict = {'C' : -1, 'X' : np.nan}\n",
    "        s = pd.to_numeric(series.replace(cat_dict))\n",
    "        return s\n",
    "    elif cat_type =='loan_purpose' :\n",
    "        cat_list_dict = {'Mobile' : ['Mobile'],\n",
    "                         'Electronics' : ['Consumer Electronics', 'Purchase of electronic equipment'],\n",
    "                         'Computers' : ['Computers'],\n",
    "                         'Audio/Video' : ['Audio/Video'],\n",
    "                         'Furniture' : ['Furniture'],\n",
    "                         'Photo / Cinema Equipment' : ['Photo / Cinema Equipment'],\n",
    "                         'Construction Materials' : ['Construction Materials'],\n",
    "                         'Repairs' : ['Repairs'],\n",
    "                         'Clothing and Accessories' : ['Clothing and Accessories'],\n",
    "                         'Other' : ['Other', 'Animals', 'Money for a third person',\n",
    "                                    'Direct Sales', 'Business development', 'Additional Service', 'Insurance',\n",
    "                                    'Weapon'],\n",
    "                         'Urgent needs' : ['Urgent needs'],\n",
    "                         'Car repairs / accessories' : ['Auto Accessories', 'Car repairs'],\n",
    "                         'Jewelry' : ['Jewelry'],\n",
    "                         'Homewares' : ['Homewares'],\n",
    "                         'Medical Supplies' : ['Medical Supplies', 'Medicine'],\n",
    "                         'Vehicles' : ['Vehicles', 'Buying a used car', 'Buying a new car'],\n",
    "                         'Sport and Leisure' : ['Sport and Leisure', 'Fitness', 'Hobby'],\n",
    "                         'House Construction / Purchase' : ['House Construction', 'Building a house or an annex', \n",
    "                                                            'Buying a home', 'Buying a holiday home / land',\n",
    "                                                            'Gasification / water supply', 'Buying a garage'],\n",
    "                         'Gardening' : ['Gardening'],\n",
    "                         'Everyday expenses' : ['Everyday expenses'],\n",
    "                         'Office Appliances' : ['Office Appliances'],\n",
    "                         'Payments on other loans' : ['Payments on other loans'],\n",
    "                         'Travel / Tourism / Wedding' : ['Tourism', 'Journey', 'Wedding / gift / holiday'],\n",
    "                         'Education' : ['Education'],\n",
    "                         #'Wedding / gift / holiday' : ['Wedding / gift / holiday'],\n",
    "                         #'Direct Sales' : ['Direct Sales'],\n",
    "                         #'Business development' : ['Business development'],\n",
    "                         #'Services' : ['Additional Service', 'Insurance'],\n",
    "                         #'Weapon' : ['Weapon'],\n",
    "                         }\n",
    "        cats = cat_list_dict.keys()\n",
    "        repl_dict = {}\n",
    "        for k, l in cat_list_dict.items():\n",
    "            if len(l) > 1 :\n",
    "                repl_dict = repl_dict | {el:k for el in l}\n",
    "        s = series.replace(repl_dict)\n",
    "        repl_dict = {el:np.nan for el in s.unique() if el not in cats}\n",
    "        s = s.replace(repl_dict)\n",
    "        return s\n",
    "    elif cat_type == 'suite_type':\n",
    "        repl_dict = {'Group of people' : 'Other',\n",
    "                     'Other_B' : 'Other',\n",
    "                     'Other_A' : 'Other',\n",
    "                    }\n",
    "        s = series.replace(repl_dict).fillna('Other')\n",
    "        return s\n",
    "    elif cat_type == 'payment_type':\n",
    "        repl_dict = {'Cash through the bank' : 'Cash',\n",
    "                     'Non-cash from your account': 'Cashless',\n",
    "                     'Cashless from the account of the employer' : 'Cashless'}\n",
    "        s = series.replace(repl_dict)\n",
    "        return s\n",
    "    else:\n",
    "        return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bc30b",
   "metadata": {},
   "source": [
    "## 3.2 Premier Prétraitement et Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc86a41",
   "metadata": {},
   "source": [
    "### 3.2.1 Le fichier: application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290cfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_application(filepath, num_rows=None, nan_as_category=True) :\n",
    "    df = pd.read_csv(filepath, nrows= num_rows)\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    print(\"....\")\n",
    "    \n",
    "    #Transform the Gender into a binary feature with Female = 1, Male = 0 and 'XNA' and 'nan's set equal to 0.5\n",
    "    #Transform the Yes/No Categories that have not yet been transformed into binary features (yes=1; no=0)\n",
    "    #Tranform the Contract type into a binary feature (cash loan=1; revolving loan=0)\n",
    "    #Transform the weekdays into numbers with Monday=0 and Sunday=6\n",
    "    #Transform the education type into levels \n",
    "    specific_categories_dict = {'CODE_GENDER'                : {'cat_type' : 'gender', 'impute_value' : 0.5},\n",
    "                                'FLAG_OWN_CAR'               : {'cat_type' : 'yes_no'},\n",
    "                                'FLAG_OWN_REALTY'            : {'cat_type' : 'yes_no'},\n",
    "                                'EMERGENCYSTATE_MODE'        : {'cat_type' : 'yes_no'},\n",
    "                                'NAME_CONTRACT_TYPE'         : {'cat_type' : 'loan'},\n",
    "                                'WEEKDAY_APPR_PROCESS_START' : {'cat_type' : 'weekday'},\n",
    "                                'NAME_EDUCATION_TYPE'        : {'cat_type' : 'education'},\n",
    "                               }\n",
    "    \n",
    "    for cat, kwargs in specific_categories_dict.items():\n",
    "        df[cat] = cat_encoder(df[cat], **kwargs)[0]\n",
    "        \n",
    "    #Reduce the categories for NAME_TYPE_SUITE into Unaccompagnied, Family and Other\n",
    "    df['NAME_TYPE_SUITE']=specific_cat_reduction(df['NAME_TYPE_SUITE'], cat_type='suite_type')\n",
    "    \n",
    "    #Transform the remaining categorical features into One Hot Features\n",
    "    df, new_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    #Correcting obviously false values to np.nan for the variables containing 'DAYS_'. \n",
    "    #They are in general referenced to the application data and therefore need to be smaller than zero.\n",
    "    #Additionally, they need to be smaller than the days since birth.\n",
    "    days_cols = [col for col in df.columns if col.startswith('DAYS_')]\n",
    "    select = (df[days_cols] > 0) | (np.array([df[col] < df['DAYS_BIRTH'] for col in days_cols]).T)\n",
    "    df[days_cols] = np.where(select, np.nan, df[days_cols])\n",
    "    \n",
    "    #Feature Engineering based on \"domain knowledge\" - features used in other kernels\n",
    "    # Some simple new features (percentages)\n",
    "    df['NEW_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['NEW_INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['NEW_INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['NEW_PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    #Drop columns that have zero variation\n",
    "    df.drop(df.columns[df.std(axis=0) == 0], axis=1, inplace=True)\n",
    "    \n",
    "    #Drop columns that where found to have little to none importance:\n",
    "    #drop_cols = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_CONT_MOBILE']\n",
    "    #df.drop(columns=drop_cols, inplace=True)\n",
    "    df.columns = pd.Index(['APPL_' + col if col not in ['SK_ID_CURR', 'TARGET'] else col for col in df.columns ])\n",
    "\n",
    "    \n",
    "    \n",
    "    print('After the first preprocesing:')\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d7aa3",
   "metadata": {},
   "source": [
    "### 3.2.2 Le fichier: bureau et bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11ae1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bureau(filepath, num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv(filepath, nrows= num_rows)\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    print(\"....\")\n",
    "    \n",
    "    ind_link = df[['SK_ID_CURR', 'SK_ID_BUREAU']]\n",
    "    \n",
    "    #The currency used is to over 99% currency 1, so will will transform this to a binary variable.\n",
    "    specific_categories_dict = {'CREDIT_CURRENCY' : {'cat_type' : 'currency'}}\n",
    "    for cat, kwargs in specific_categories_dict.items():\n",
    "        df[cat] = cat_encoder(df[cat], **kwargs)[0]\n",
    "    \n",
    "    #Reduce the number of categories for the credit type, but before that count the number of different credit types\n",
    "    type_agg = df.groupby(by='SK_ID_CURR').agg({'CREDIT_TYPE' : 'nunique'}).rename(\n",
    "                columns={'CREDIT_TYPE' : 'CREDIT_TYPE_NUNIQUE'})\n",
    "    df['CREDIT_TYPE']= specific_cat_reduction(df['CREDIT_TYPE'], 'bureau_credit_type')\n",
    "    \n",
    "    #Dropping the data for which the credit update is after the application data for the current loan.\n",
    "    select = df['DAYS_CREDIT_UPDATE'] <= 0\n",
    "    df = df[select]\n",
    "    \n",
    "    #Setting the time columns that are more than 100 years (=-36524.3 days) before the time of the application to np.nan\n",
    "    days_cols = [col for col in df.columns if col.startswith('DAYS_')]\n",
    "    select = (df[days_cols] <= -36524.3) \n",
    "    df[days_cols] = np.where(select, np.nan, df[days_cols])\n",
    "    \n",
    "    #Correcting the entries that are active, have an actual credit enddate, but not a planned credit enddate to 'Closed'\n",
    "    select = (df['CREDIT_ACTIVE']=='Active') & (df['DAYS_ENDDATE_FACT'].notnull()) & (df['DAYS_CREDIT_ENDDATE'].isnull())\n",
    "    df['CREDIT_ACTIVE'] = np.where(select, 'Closed', df['CREDIT_ACTIVE'])\n",
    "    \n",
    "    #Feature Engineering: Duration of the Credit\n",
    "    df['NEW_DURATION_CREDIT_DAYS'] = df['DAYS_ENDDATE_FACT'].fillna(df['DAYS_CREDIT_ENDDATE']) - df['DAYS_CREDIT']\n",
    "    select = df['NEW_DURATION_CREDIT_DAYS'] < 0\n",
    "    df['NEW_DURATION_CREDIT_DAYS'] = np.where(select, np.nan, df['NEW_DURATION_CREDIT_DAYS'])\n",
    "    \n",
    "    #Transform the categorical features into One Hot Features\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # Aggregations for the numerical features\n",
    "    num_aggregations = {\n",
    "        'CREDIT_CURRENCY' : ['mean'],\n",
    "        'NEW_DURATION_CREDIT_DAYS' : ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var', 'size'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['sum'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "    }\n",
    "    \n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols: \n",
    "        cat_aggregations[cat] = ['mean', 'sum']\n",
    "    \n",
    "    agg_df = df.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    agg_df.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in agg_df.columns.tolist()])\n",
    "    agg_df = agg_df.join(type_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del type_agg\n",
    "    gc.collect()\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = df[df['CREDIT_ACTIVE_Active'] == 1]\n",
    "    red_num_aggs = dict(num_aggregations)\n",
    "    red_num_aggs.pop('DAYS_ENDDATE_FACT')\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(red_num_aggs)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    agg_df = agg_df.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    ## Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = df[df['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    red_num_aggs = dict(num_aggregations)\n",
    "    red_num_aggs.pop('DAYS_CREDIT_ENDDATE')\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(red_num_aggs)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    agg_df = agg_df.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del closed, closed_agg, df\n",
    "    gc.collect()\n",
    "    #return bureau_agg\n",
    "    \n",
    "    #Drop columns that have zero variation\n",
    "    agg_df.drop(agg_df.columns[agg_df.std(axis=0) == 0], axis=1, inplace=True)\n",
    "    \n",
    "    print('After the first preprocesing and aggregation:')\n",
    "    print(f\"The data contains {len(agg_df)} samples,\")\n",
    "    print(f\"and there are {agg_df.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    return agg_df, ind_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914f031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bureau_balance(filepath, num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv(filepath, nrows= num_rows)\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    print(\"....\")\n",
    "    \n",
    "    df['STATUS']= specific_cat_reduction(df['STATUS'], 'bureau_balance_status')\n",
    "    #impute the values for the unknown status (=np.nan) with the value of the balance the month before\n",
    "    df = df.sort_values(by=['SK_ID_BUREAU', 'MONTHS_BALANCE'])\n",
    "    df['STATUS'] = df.groupby('SK_ID_BUREAU')['STATUS'].ffill()\n",
    "    \n",
    "    #Determine if the Account was Closed or Not and the month for which the account was closed \n",
    "    #status_agg = df.groupby('SK_ID_BUREAU')[['SK_ID_BUREAU']].first().reset_index(drop=True)\n",
    "    #status_agg = status_agg.join(df[df['STATUS']==-1].groupby('SK_ID_BUREAU').first(), how='left', on='SK_ID_BUREAU')\n",
    "    #status_agg.columns = pd.Index([e + '_CLOSED' for e in status_agg.columns])\n",
    "    #status_agg['STATUS_CLOSED'] = status_agg['STATUS_CLOSED'].fillna(0).replace(-1, 1)\n",
    "    \n",
    "    \n",
    "    num_aggregations = {\n",
    "                        'MONTHS_BALANCE' : ['min', 'max'],\n",
    "                        'STATUS' : ['max', 'min'],\n",
    "                        }\n",
    "    \n",
    "    agg_df = df.groupby('SK_ID_BUREAU').agg(num_aggregations)\n",
    "    agg_df.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in agg_df.columns.tolist()])\n",
    "    #if the minium status is -1, this means that the credit is closed.\n",
    "    agg_df['BURO_STATUS_MIN'] = agg_df['BURO_STATUS_MIN'].apply(lambda x : 1 if x==-1 else 0)\n",
    "    agg_df = agg_df.rename(columns={'BURO_STATUS_MIN':'BURO_STATUS_CLOSED'})\n",
    "    #if the maximum status is -1, replace the value with np.nan. One does not know what the max. balance status was.\n",
    "    agg_df['BURO_STATUS_MAX'] = agg_df['BURO_STATUS_MAX'].replace(-1, np.nan)\n",
    "    # determine the month the account was closed\n",
    "    closed = df['STATUS'] == -1\n",
    "    agg_df = agg_df.join(df[closed].groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].min().rename(\n",
    "                         'BURO_MONTHS_BALANCE_CLOSED'),\n",
    "                         how='left', on='SK_ID_BUREAU')\n",
    "    \n",
    "    num_aggregations = {\n",
    "                        'MONTHS_BALANCE' : ['size'],\n",
    "                        'STATUS' : ['mean'],\n",
    "                        }\n",
    "    \n",
    "    #determine the actual time for which one has information on the balance\n",
    "    df = df.drop(df[closed].index).dropna(subset=['STATUS'])\n",
    "    status_agg = df.groupby('SK_ID_BUREAU').agg(num_aggregations)\n",
    "    status_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in status_agg.columns.tolist()])\n",
    "    agg_df = agg_df.join(status_agg, how='left', on='SK_ID_BUREAU')\n",
    "    \n",
    "    del closed, status_agg, df\n",
    "    gc.collect()\n",
    "        \n",
    "    print('After the first preprocesing and aggregation:')\n",
    "    print(f\"The data contains {len(agg_df)} samples,\")\n",
    "    print(f\"and there are {agg_df.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3843b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_bureau_balance(bureau, balance, bureau_balance_link):\n",
    "    num_aggregations = {\n",
    "                        'BURO_MONTHS_BALANCE_MIN' : ['min'],\n",
    "                        'BURO_MONTHS_BALANCE_MAX' : ['max'],\n",
    "                        'BURO_STATUS_MAX' : ['max'],\n",
    "                        'BURO_STATUS_CLOSED' : ['mean', 'sum'],\n",
    "                        'BURO_MONTHS_BALANCE_CLOSED' : ['mean'],\n",
    "                        'BURO_MONTHS_BALANCE_SIZE' : ['mean'],\n",
    "                        'BURO_STATUS_MEAN' : ['min','max','mean'],\n",
    "                        }\n",
    "\n",
    "    df = bureau_balance_link.join(balance, how='left', on='SK_ID_BUREAU')\n",
    "    balance_agg = df.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    balance_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in balance_agg.columns.tolist()])\n",
    "    \n",
    "    balance_agg = bureau.join(balance_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('After aggregating bureau and bureau_balance:')\n",
    "    print(f\"The data contains {len(balance_agg)} samples,\")\n",
    "    print(f\"and there are {balance_agg.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    return balance_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed2138",
   "metadata": {},
   "source": [
    "### 3.2.3 Le fichier: previous_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b3ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def preprocess_previous_applications(filepath, num_rows = None, nan_as_category = True, feature_drop=True):\n",
    "    df = pd.read_csv(filepath, nrows= num_rows)\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    print(\"....\")\n",
    "    \n",
    "    #Correcting obviously false values to np.nan for the variables containing 'DAYS_'. \n",
    "    #We limit them to being less than 100 years in the future and require them to be after decision date for the loan.\n",
    "    days_cols = [col for col in df.columns if col.startswith('DAYS_')]\n",
    "    select = (df[days_cols] > 35624.3) | (np.array([df[col] < df['DAYS_DECISION'] for col in days_cols]).T)\n",
    "    df[days_cols] = np.where(select, np.nan, df[days_cols])\n",
    "    \n",
    "    #Treating categories that can transform to numerical values\n",
    "    specific_categories_dict = {'FLAG_LAST_APPL_PER_CONTRACT': {'cat_type' : 'yes_no'},\n",
    "                                'WEEKDAY_APPR_PROCESS_START' : {'cat_type' : 'weekday'},\n",
    "                                'NAME_YIELD_GROUP'           : {'cat_type' : 'yield_group'},\n",
    "                               }\n",
    "    \n",
    "    for cat, kwargs in specific_categories_dict.items():\n",
    "        df[cat] = cat_encoder(df[cat], **kwargs)[0]\n",
    "        \n",
    "    #Reduce the categories for NAME_TYPE_SUITE into Unaccompagnied, Family, Childre, Partner/Spouse and Other\n",
    "    df['NAME_TYPE_SUITE']=specific_cat_reduction(df['NAME_TYPE_SUITE'], cat_type='suite_type')\n",
    "        \n",
    "    #A small category reduction for 'NAME_PAYMENT_TYPE':\n",
    "    df['NAME_PAYMENT_TYPE'] = specific_cat_reduction(df['NAME_PAYMENT_TYPE'], cat_type='payment_type')\n",
    "    \n",
    "    #The information of category 'NAME_CONTRACT_STATUS' is basically contained in 'CODE_REJECT_REASON'.\n",
    "    #We can combine these two into a single category.\n",
    "    select1 = df['NAME_CONTRACT_STATUS'] == 'Approved' #APPROVED\n",
    "    select2 = df['NAME_CONTRACT_STATUS'] == 'Canceled' #CANCELED\n",
    "    select3 = df['CODE_REJECT_REASON'] == 'XAP' #XNA\n",
    "    df['CODE_CONTRACT_STATUS'] = np.where(select1, 'APPROVED', \n",
    "                                 np.where(select2, 'CANCELED', \n",
    "                                 np.where(select3, 'XNA', df['CODE_REJECT_REASON'])))\n",
    "    df.drop(columns=['NAME_CONTRACT_STATUS', 'CODE_REJECT_REASON'], inplace=True)\n",
    "\n",
    "    #The categories: 'NAME_CASH_LOAN_PURPOSE' and 'NAME_GOODS_CATEGORY' are mutually explusive\n",
    "    #The goods categories is for Consumer loans. We combine these two categories to create a general purpose category.\n",
    "    #Since there are many categories we will do some category reduction.\n",
    "    df['LOAN_PURPOSE'] = specific_cat_reduction(df['NAME_CASH_LOAN_PURPOSE'], cat_type = 'loan_purpose').fillna(\n",
    "                         specific_cat_reduction(df['NAME_GOODS_CATEGORY'], cat_type = 'loan_purpose'))\n",
    "    df.drop(columns=['NAME_CASH_LOAN_PURPOSE', 'NAME_GOODS_CATEGORY'], inplace=True)\n",
    "    \n",
    "    #The category 'NAME_PORTFOLIO' does not seem to bring much more information. \n",
    "    #It is XNA, when the yield group 'NAME_YIELD_GROUP' is XNA. Otherwise it corresponds to 'NAME_CONTRACT_TYPE', \n",
    "    #with the exception of 'Cars', which is less than 500 samples. (we can drop this category)\n",
    "    df.drop(columns=['NAME_PORTFOLIO'], inplace=True)\n",
    "    \n",
    "    #The category 'PRODUCT_COMBINATION' is essentially 'NAME_CONTRACT_TYPE' + 'NAME_PRODUCT_TYPE' + 'NAME_YIELD_GROUP'\n",
    "    #These is additional information for the Consumer loans ('househhold', 'mobile', 'industry', 'other') \n",
    "    #and 'with / without interest'. \n",
    "    #At the same time the 'NAME_PRODUCT_TYPE' only refers to Cash and Revolving loans.\n",
    "    select = df['NAME_CONTRACT_TYPE'] == 'Consumer loans'\n",
    "    df['NAME_PRODUCT_TYPE'] = np.where(select, df['PRODUCT_COMBINATION'], df['NAME_PRODUCT_TYPE'])\n",
    "    df.drop(columns=['PRODUCT_COMBINATION'], inplace=True)\n",
    "\n",
    "    #We will flag the entries without a yield group.\n",
    "    df['FLAG_YIELD_GROUP'] = df['NAME_YIELD_GROUP'].isnull().astype(int)\n",
    "\n",
    "    #categories that were found to have very little importance\n",
    "    if feature_drop:\n",
    "        df.drop(columns=['CHANNEL_TYPE', 'NAME_SELLER_INDUSTRY', 'LOAN_PURPOSE',\n",
    "                         'NAME_PRODUCT_TYPE'],\n",
    "                inplace=True)\n",
    "    \n",
    "    #Transform the remaining categorical features into One Hot Features\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)    \n",
    "    \n",
    "    # Add feature: value ask / value received percentage\n",
    "    df['NEW_APPL_CREDIT_PERC'] = df['AMT_APPLICATION'] / df['AMT_CREDIT']\n",
    "    \n",
    "\n",
    "        \n",
    "    # Previous applications numeric features, including the categorical features transformed into numbers\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'NEW_APPL_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'WEEKDAY_APPR_PROCESS_START' : ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'RATE_INTEREST_PRIMARY' : ['mean'],\n",
    "        'RATE_INTEREST_PRIVILEGED' : ['mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'SELLERPLACE_AREA' : ['mean', pd.Series.mode],\n",
    "        'NAME_YIELD_GROUP' : ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'DAYS_FIRST_DRAWING' : ['min', 'max', 'mean'],\n",
    "        'DAYS_FIRST_DUE' : ['min', 'max', 'mean'],\n",
    "        'DAYS_LAST_DUE_1ST_VERSION' : ['min', 'max', 'mean'],\n",
    "        'DAYS_LAST_DUE' : ['min', 'max', 'mean'],\n",
    "        'DAYS_TERMINATION' : ['min', 'max', 'mean'],\n",
    "        'NFLAG_INSURED_ON_APPROVAL' : ['mean', 'sum'],\n",
    "        'FLAG_YIELD_GROUP' : ['mean']\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean', 'sum']\n",
    "        \n",
    "    agg_df = df.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    agg_df.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in agg_df.columns.tolist()])\n",
    "    \n",
    "    num_aggregations = {\n",
    "            'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n",
    "            'NEW_APPL_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "            'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n",
    "            'HOUR_APPR_PROCESS_START': ['min', 'max'],\n",
    "            'WEEKDAY_APPR_PROCESS_START' : ['min', 'max', 'mean'],\n",
    "            'DAYS_DECISION': ['min', 'max', 'sum'],\n",
    "            'NAME_YIELD_GROUP' : ['min', 'max', 'mean'],\n",
    "        }   \n",
    "    \n",
    "    \n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = df[df['CODE_CONTRACT_STATUS_APPROVED'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    agg_df = agg_df.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    # After some tests they don's seem to contribute much therefore we will take this part out.\n",
    "    if not feature_drop :\n",
    "        select_refused = (df['CODE_CONTRACT_STATUS_APPROVED'] == 0) | (df['CODE_CONTRACT_STATUS_CANCELED'] == 0) | \\\n",
    "                         (df['CODE_CONTRACT_STATUS_CLIENT'] == 0) | (df['CODE_CONTRACT_STATUS_XNA'] == 0) \n",
    "        refused = df[select_refused]\n",
    "        refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "        refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "        agg_df = agg_df.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "        del refused, refused_agg\n",
    "    \n",
    "    del approved, approved_agg, df\n",
    "    gc.collect()\n",
    "    \n",
    "    #Drop columns that have zero variation\n",
    "    for col in agg_df.select_dtypes(include = 'object'):\n",
    "        try:\n",
    "            agg_df[col] = pd.to_numeric(agg_df[col])\n",
    "        except:\n",
    "            print(f\"ATTENTION : Column {col} is coerced to be numeric!\")\n",
    "            agg_df[col] = pd.to_numeric(agg_df[col], errors='coerce')\n",
    "            \n",
    "    agg_df.drop(agg_df.columns[agg_df.std(axis=0) == 0], axis=1, inplace=True)\n",
    "    \n",
    "    print('After the first preprocesing and aggregation:')\n",
    "    print(f\"The data contains {len(agg_df)} samples,\")\n",
    "    print(f\"and there are {agg_df.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab7e40",
   "metadata": {},
   "source": [
    "### 3.2.4 Le fichier: POS_CASH_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eef5363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pos_cash(filepath, num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv(filepath, nrows= num_rows)\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    print(\"....\")\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'sum', 'var', 'size'],\n",
    "        'SK_DPD':         ['max', 'mean'],\n",
    "        'SK_DPD_DEF':     ['max', 'mean'],\n",
    "        'CNT_INSTALMENT': ['mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE' : ['mean','sum'],\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean', 'sum']\n",
    "    \n",
    "    agg_df = df.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in agg_df.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    #agg_df['POS_COUNT'] = df.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    #Drop columns that have zero variation\n",
    "    agg_df.drop(agg_df.columns[agg_df.std(axis=0) == 0], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    print('After the first preprocesing and aggregation:')\n",
    "    print(f\"The data contains {len(agg_df)} samples,\")\n",
    "    print(f\"and there are {agg_df.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b806d",
   "metadata": {},
   "source": [
    "### 3.2.5 Le fichier: installments_payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dafaed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_installments_payments(filepath, num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv(filepath, nrows= num_rows)\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    print(\"....\")\n",
    "\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    df['NEW_PAYMENT_PERC'] = df['AMT_PAYMENT'] / df['AMT_INSTALMENT']\n",
    "    df['NEW_PAYMENT_DIFF'] = df['AMT_INSTALMENT'] - df['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    df['NEW_DPD'] = df['DAYS_ENTRY_PAYMENT'] - df['DAYS_INSTALMENT']\n",
    "    df['NEW_DBD'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n",
    "    df['NEW_DPD'] = df['NEW_DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    df['NEW_DBD'] = df['NEW_DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'NEW_DPD': ['max', 'mean', 'sum'],\n",
    "        'NEW_DBD': ['max', 'mean', 'sum'],\n",
    "        'NEW_PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'NEW_PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    \n",
    "    agg_df = df.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in agg_df.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    agg_df['INSTAL_COUNT'] = df.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    #Drop columns that have zero variation\n",
    "    agg_df.drop(agg_df.columns[agg_df.std(axis=0) == 0], axis=1, inplace=True)\n",
    "    \n",
    "    print('After the first preprocesing and aggregation:')\n",
    "    print(f\"The data contains {len(agg_df)} samples,\")\n",
    "    print(f\"and there are {agg_df.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01800605",
   "metadata": {},
   "source": [
    "### 3.2.6 Le fichier: credit_card_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50362fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_credit_card_balance(filepath, num_rows = None, nan_as_category = True):\n",
    "    df = pd.read_csv(filepath, nrows= num_rows)\n",
    "    print(f\"The data contains {len(df)} samples.\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    print(\"....\")\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "\n",
    "    # Percentage and difference paid each (amount paid and balance amount)\n",
    "    df['NEW_PAYMENT_PERC'] = df['AMT_PAYMENT_CURRENT'] / df['AMT_BALANCE']\n",
    "    df['NEW_PAYMENT_DIFF'] = df['AMT_BALANCE'] - df['AMT_PAYMENT_CURRENT']\n",
    "    \n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'AMT_BALANCE' :   ['min', 'max', 'mean', 'sum', 'var'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL' : ['min', 'max', 'mean', 'sum', 'var'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT' : ['max','mean','sum','var'],\n",
    "        'AMT_DRAWINGS_CURRENT' : ['max','mean','sum','var'],\n",
    "        'AMT_DRAWINGS_OTHER_CURRENT' : ['max','mean','sum','var'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT' : ['max','mean','sum','var'],\n",
    "        'AMT_INST_MIN_REGULARITY' : ['min','max','mean','sum','var'],\n",
    "        'AMT_PAYMENT_CURRENT' :  ['min','max','mean','sum','var'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT' : ['min','max','mean','sum','var'],\n",
    "        'AMT_RECEIVABLE_PRINCIPAL' : ['min','max','mean','sum','var'],\n",
    "        'AMT_RECIVABLE' : ['min','max','mean','sum','var'],\n",
    "        'AMT_TOTAL_RECEIVABLE' : ['min','max','mean','sum','var'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT' : ['mean', 'sum'],\n",
    "        'CNT_DRAWINGS_CURRENT' : ['mean', 'sum'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT' : ['mean', 'sum'],\n",
    "        'CNT_INSTALMENT_MATURE_CUM' : ['mean','sum'],\n",
    "        'NEW_PAYMENT_PERC' : ['max', 'mean', 'sum', 'var'],\n",
    "        'NEW_PAYMENT_DIFF' : ['max', 'mean', 'sum', 'var'],\n",
    "        'SK_DPD':         ['max', 'mean'],\n",
    "        'SK_DPD_DEF':     ['max', 'mean'],\n",
    "\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean', 'sum']\n",
    "    \n",
    "    agg_df = df.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in agg_df.columns.tolist()])\n",
    "    #Count credit card balance\n",
    "    agg_df['CC_COUNT'] = df.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    #Drop columns that have zero variation\n",
    "    agg_df.drop(agg_df.columns[agg_df.std(axis=0) == 0], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    print('After the first preprocesing and aggregation:')\n",
    "    print(f\"The data contains {len(agg_df)} samples,\")\n",
    "    print(f\"and there are {agg_df.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d9e34",
   "metadata": {},
   "source": [
    "### 3.2.7 Fonction pour l'Aggrégation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bedd6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_application(application, other):\n",
    "    df = application.join(other, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    print('After joining the two datasets:')\n",
    "    print(f\"The data contains {len(df)} samples,\")\n",
    "    print(f\"and there are {df.shape[1]} features in the dataset.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513676b",
   "metadata": {},
   "source": [
    "## 3.3 Import des Données et Execution du Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a21d5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortcut_path(filepath):\n",
    "    shell = win32com.client.Dispatch(\"WScript.Shell\")\n",
    "    shortcut = shell.CreateShortCut(filepath)\n",
    "    return shortcut.Targetpath +'\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67f8530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "    filenames = ['application_train', 'application_test', 'previous_application',\n",
    "                 'credit_card_balance', 'POS_CASH_balance', 'installments_payments',\n",
    "                 'bureau', 'bureau_balance']\n",
    "    preprocessing_dict = {'application_train' : preprocess_application,\n",
    "                          'application_test' : preprocess_application,\n",
    "                          'previous_application' : lambda x: preprocess_previous_applications(x, feature_drop=False),\n",
    "                          'credit_card_balance' : preprocess_credit_card_balance,\n",
    "                          'POS_CASH_balance' : preprocess_pos_cash,\n",
    "                          'installments_payments' : preprocess_installments_payments,\n",
    "                          'bureau' : preprocess_bureau,\n",
    "                          'bureau_balance' : preprocess_bureau_balance}\n",
    "    data = {}\n",
    "    for filename in filenames:\n",
    "        filepath = path + filename +'.csv'\n",
    "        print(f\"Loading and Preprocessing {filename.upper()} ...\")\n",
    "        if filename != 'bureau':\n",
    "            data[filename] = preprocessing_dict[filename](filepath)\n",
    "        else:\n",
    "            data[filename], bureau_link = preprocessing_dict[filename](filepath)\n",
    "        print()\n",
    "    data['bureau'] = aggregate_bureau_balance(data['bureau'], data['bureau_balance'], bureau_link)\n",
    "    print(50*'*')\n",
    "    print(\"Joining the DATA\")\n",
    "    print()\n",
    "    application = data['application_train']\n",
    "    application_submit = data['application_test']\n",
    "    for other in ['previous_application', 'credit_card_balance', 'POS_CASH_balance', 'installments_payments', 'bureau'] :\n",
    "        print(f\"Joining APPLICATION with {filename.upper()} data :\")\n",
    "        application = join_with_application(application, data[other])\n",
    "        print()\n",
    "        print(f\"Joining APPLICATION_SUBMIT with {filename.upper()} data :\")\n",
    "        application_submit = join_with_application(application_submit, data[other])\n",
    "        print()\n",
    "    print(50*'*')\n",
    "    print(\"DONE\")\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return application, application_submit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe0729",
   "metadata": {},
   "source": [
    "# 4. Import des Données et Exécution du Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bafdd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "DATAFOLDER = '\\\\DataShortcut.lnk'\n",
    "filepath = get_shortcut_path(PATH+DATAFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86166bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Preprocessing APPLICATION_TRAIN ...\n",
      "The data contains 307511 samples.\n",
      "and there are 122 features in the dataset.\n",
      "....\n",
      "After the first preprocesing:\n",
      "The data contains 307511 samples.\n",
      "and there are 237 features in the dataset.\n",
      "\n",
      "Loading and Preprocessing APPLICATION_TEST ...\n",
      "The data contains 48744 samples.\n",
      "and there are 121 features in the dataset.\n",
      "....\n",
      "After the first preprocesing:\n",
      "The data contains 48744 samples.\n",
      "and there are 223 features in the dataset.\n",
      "\n",
      "Loading and Preprocessing PREVIOUS_APPLICATION ...\n",
      "The data contains 1670214 samples.\n",
      "and there are 37 features in the dataset.\n",
      "....\n",
      "ATTENTION : Column PREV_SELLERPLACE_AREA_MODE is coerced to be numeric!\n",
      "After the first preprocesing and aggregation:\n",
      "The data contains 338857 samples,\n",
      "and there are 266 features in the dataset.\n",
      "\n",
      "Loading and Preprocessing CREDIT_CARD_BALANCE ...\n",
      "The data contains 3840312 samples.\n",
      "and there are 23 features in the dataset.\n",
      "....\n",
      "After the first preprocesing and aggregation:\n",
      "The data contains 103558 samples,\n",
      "and there are 94 features in the dataset.\n",
      "\n",
      "Loading and Preprocessing POS_CASH_BALANCE ...\n",
      "The data contains 10001358 samples.\n",
      "and there are 8 features in the dataset.\n",
      "....\n",
      "After the first preprocesing and aggregation:\n",
      "The data contains 337252 samples,\n",
      "and there are 31 features in the dataset.\n",
      "\n",
      "Loading and Preprocessing INSTALLMENTS_PAYMENTS ...\n",
      "The data contains 13605401 samples.\n",
      "and there are 8 features in the dataset.\n",
      "....\n",
      "After the first preprocesing and aggregation:\n",
      "The data contains 339587 samples,\n",
      "and there are 25 features in the dataset.\n",
      "\n",
      "Loading and Preprocessing BUREAU ...\n",
      "The data contains 1716428 samples.\n",
      "and there are 17 features in the dataset.\n",
      "....\n",
      "After the first preprocesing and aggregation:\n",
      "The data contains 305810 samples,\n",
      "and there are 112 features in the dataset.\n",
      "\n",
      "Loading and Preprocessing BUREAU_BALANCE ...\n",
      "The data contains 27299925 samples.\n",
      "and there are 3 features in the dataset.\n",
      "....\n",
      "After the first preprocesing and aggregation:\n",
      "The data contains 817395 samples,\n",
      "and there are 7 features in the dataset.\n",
      "\n",
      "After aggregating bureau and bureau_balance:\n",
      "The data contains 305810 samples,\n",
      "and there are 122 features in the dataset.\n",
      "**************************************************\n",
      "Joining the DATA\n",
      "\n",
      "Joining APPLICATION with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 307511 samples,\n",
      "and there are 503 features in the dataset.\n",
      "\n",
      "Joining APPLICATION_SUBMIT with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 48744 samples,\n",
      "and there are 489 features in the dataset.\n",
      "\n",
      "Joining APPLICATION with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 307511 samples,\n",
      "and there are 597 features in the dataset.\n",
      "\n",
      "Joining APPLICATION_SUBMIT with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 48744 samples,\n",
      "and there are 583 features in the dataset.\n",
      "\n",
      "Joining APPLICATION with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 307511 samples,\n",
      "and there are 628 features in the dataset.\n",
      "\n",
      "Joining APPLICATION_SUBMIT with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 48744 samples,\n",
      "and there are 614 features in the dataset.\n",
      "\n",
      "Joining APPLICATION with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 307511 samples,\n",
      "and there are 653 features in the dataset.\n",
      "\n",
      "Joining APPLICATION_SUBMIT with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 48744 samples,\n",
      "and there are 639 features in the dataset.\n",
      "\n",
      "Joining APPLICATION with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 307511 samples,\n",
      "and there are 775 features in the dataset.\n",
      "\n",
      "Joining APPLICATION_SUBMIT with BUREAU_BALANCE data :\n",
      "After joining the two datasets:\n",
      "The data contains 48744 samples,\n",
      "and there are 761 features in the dataset.\n",
      "\n",
      "**************************************************\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "application, application_submit = load_and_preprocess_data(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e637d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 775)\n",
      "(48744, 761)\n"
     ]
    }
   ],
   "source": [
    "print(application.shape)\n",
    "print(application_submit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ebdc79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24825 clients in the dataset defaulted, 282686 did not.\n",
      "That's 8.78%.\n"
     ]
    }
   ],
   "source": [
    "y = application['TARGET']\n",
    "\n",
    "print(f\"{len(application[y==1])} clients in the dataset defaulted, {len(application[y==0])} did not.\")\n",
    "print(f\"That's {round(100*len(application[y==1])/len(application[y==0]), 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb1021",
   "metadata": {},
   "source": [
    "# 5. Export des Données Prétraitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be185c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = True\n",
    "sample = True\n",
    "if export:\n",
    "    PATH = os.getcwd()\n",
    "    DATAFOLDER = '\\\\Data\\\\'\n",
    "\n",
    "    sufffix=''\n",
    "    if sample:\n",
    "        for filename, data in zip(['data_processed_min', 'data_processed_submit_min'], \n",
    "                                  [application.sample(frac=0.09), application_submit.sample(frac=0.5)]) :\n",
    "            data.to_csv(PATH + DATAFOLDER + filename + '.csv')\n",
    "    else:\n",
    "        for filename, data in zip(['data_processed', 'data_processed_submit'], \n",
    "                                  [application, application_submit]) :\n",
    "            data.to_csv(PATH + DATAFOLDER + filename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cff969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
